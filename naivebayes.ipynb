{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes as Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, hamming_loss, f1_score, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(strip_accents='ascii', token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', \n",
    "                     lowercase=True, stop_words='english')\n",
    "\n",
    "frames = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nb(verbose = False):\n",
    "    f1_macro, f1_weighted, f1_micro,  hamming, stracc, multiacc = [[] for i in range(6)]\n",
    "    for fold in range(5):\n",
    "        train = pd.read_csv('dataset/'+str(fold)+'/train.tsv', sep='\\t', header=None)\n",
    "        train.columns = [\"ImageID\", \"Headline\", \"text_b\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\",\"9\"]\n",
    "    #     train.head()\n",
    "        train['Headline'] = train['Headline'].map(lambda h : clean_text(h))\n",
    "        test = pd.read_csv('dataset/'+str(fold)+'/test_labeled.tsv', sep='\\t', header=None)\n",
    "        test.columns = [\"ImageID\", \"Headline\", \"text_b\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "        test['Headline'] = test['Headline'].map(lambda h : clean_text(h))\n",
    "        test.head()\n",
    "        X_train = train['Headline']\n",
    "        X_test = test['Headline']\n",
    "        X_train_cv = cv.fit_transform(X_train)\n",
    "        X_test_cv = cv.transform(X_test)\n",
    "        word_freq_df = pd.DataFrame(X_train_cv.toarray(), columns=cv.get_feature_names())\n",
    "        top_words_df = pd.DataFrame(word_freq_df.sum()).sort_values(0, ascending=False)\n",
    "    #     word_freq_df.head()\n",
    "        top_words_df.head()\n",
    "\n",
    "        results = pd.DataFrame(columns = frames)\n",
    "        preds_df = pd.DataFrame(columns = frames)\n",
    "        for f in frames:\n",
    "            naive_bayes = MultinomialNB()\n",
    "            y_train = train[f]\n",
    "            naive_bayes.fit(X_train_cv, y_train)\n",
    "            predictions = naive_bayes.predict(X_test_cv)\n",
    "            y_test = test[f]\n",
    "    #         print(\"fold: \", fold, \"f \", f)\n",
    "    #         print(y_test)\n",
    "    #         print(predictions)\n",
    "            results[f] = [accuracy_score(y_test, predictions),\n",
    "                         precision_score(y_test, predictions),\n",
    "                         recall_score(y_test, predictions),\n",
    "                         f1_score(y_test, predictions, average=\"macro\"),\n",
    "                         f1_score(y_test, predictions, average=\"micro\")]\n",
    "            preds_df[f] = predictions\n",
    "\n",
    "        results[\"Metric\"] = pd.Series([\"Accuracy\", \"Precision\", \"Recall\", \"F1_Macro\", \"F1_Micro\"])\n",
    "        results.set_index(\"Metric\", inplace=True)\n",
    "    #     results['Average'] = results.mean(axis=1)\n",
    "    #     multilabel_confusion_matrix(test.iloc[:,3:], preds_df)\n",
    "\n",
    "        # print scores\n",
    "        rd = 3\n",
    "        f1_macro.append(f1_score(test.iloc[:,3:], preds_df, average='macro').round(rd))\n",
    "        f1_weighted.append(f1_score(test.iloc[:,3:], preds_df, average='weighted').round(rd))\n",
    "        f1_micro.append(f1_score(test.iloc[:,3:], preds_df, average='micro').round(rd))\n",
    "        hamming.append(np.round(hamming_loss(test.iloc[:,3:], preds_df),rd))\n",
    "        stracc.append(np.round(np.sum(np.equal(test.iloc[:,3:], preds_df).all(1))/preds_df.shape[0],rd))\n",
    "#         print(\"F1-Macro: \", )\n",
    "#         print(\"F1-Weighted: \", )\n",
    "#         print(\"F1-Micro: \", )\n",
    "#         print(\"Hamming loss: \", )\n",
    "#         print(\"Exact match: \", )\n",
    "        multiple_frame_articles_bool = np.sum(test.iloc[:,3:], axis=1) > 1.0\n",
    "        equality_all_frames = np.equal(test.iloc[:,3:], preds_df)\n",
    "        results_multiple = equality_all_frames.loc[multiple_frame_articles_bool]\n",
    "        number_multiple = results_multiple.shape[0]\n",
    "        match_multiple = np.sum(results_multiple.all(1))/number_multiple\n",
    "        \n",
    "        print(\"Number of articles with multiple frames: \", number_multiple)\n",
    "        multiacc.append(np.round(match_multiple,rd))\n",
    "#         print(\"Exact match among those: \", )\n",
    "    return np.array([f1_macro, f1_weighted, f1_micro,  hamming, stracc, multiacc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with multiple frames:  59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with multiple frames:  64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with multiple frames:  70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with multiple frames:  66\n",
      "Number of articles with multiple frames:  60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/share/pkg.7/python3/3.7.3/install/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "r = run_nb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1_macro       0.537\n",
       "f1_weighted    0.667\n",
       "f1_micro       0.698\n",
       "hamming        0.073\n",
       "stracc         0.502\n",
       "multiacc       0.291\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(r.T, columns=[\"f1_macro\", \"f1_weighted\", \"f1_micro\", \n",
    "                                 \"hamming\", \"stracc\", \"multiacc\"]).mean(axis=0).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
